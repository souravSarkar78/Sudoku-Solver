{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94770587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pan card fraud detection- DataFlair\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import Augmenter as ag\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt              \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd2f613",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_limit = 15\n",
    "def augment_image(image):\n",
    "    \"\"\"Performs image augmentation operations\"\"\"\n",
    "    choice_X = random.randint(-augmentation_limit, augmentation_limit)\n",
    "    choice_Y = random.randint(-augmentation_limit, augmentation_limit)\n",
    "    \n",
    "    return ag.shift_image(image, choice_X, choice_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f473bdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7', '3', '6', '1', '0', '4', '8', '2', '5', '9']\n"
     ]
    }
   ],
   "source": [
    "img_size = 48\n",
    "number_of_augmentation = 23\n",
    "datadir = r'images'    # root data directiory\n",
    "CATEGORIES = os.listdir(datadir)\n",
    "print(CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5f7a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.arange(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b567738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [str(i) for i in l]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4b44ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 classes: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1039/1039 [00:17<00:00, 58.60it/s]\n",
      " 23%|██▎       | 233/1035 [00:03<00:12, 65.69it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-52fcca11b68f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCATEGORIES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCATEGORIES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatadir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-52fcca11b68f>\u001b[0m in \u001b[0;36mPreProcess\u001b[0;34m(img_size, path)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mresized_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"images\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresized_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Augmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Preprocessing Function\n",
    "def PreProcess(img_size, path):\n",
    "    \"\"\"This function reads images from the given folders subfolder \n",
    "        and returns a normalized array along with their respective classes\"\"\"\n",
    "    x, y = [], []\n",
    "    CATEGORIES = np.arange(0, 10)\n",
    "    CATEGORIES = [str(i) for i in CATEGORIES]\n",
    "    print(\"Found {} classes: {}\".format(len(CATEGORIES), CATEGORIES))\n",
    "    \n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(datadir, str(category))\n",
    "        classIndex = CATEGORIES.index(category)\n",
    "        \n",
    "        # Reads each image from the directory\n",
    "        for imgs in tqdm(os.listdir(path)):\n",
    "            img_arr = cv2.imread(os.path.join(path, imgs), 0)\n",
    "#             _, img_arr = cv2.threshold(img_arr, 70, 255, cv2.THRESH_BINARY_INV)\n",
    "            # resize the image\n",
    "            resized_array = cv2.resize(img_arr, (img_size, img_size))\n",
    "            cv2.imshow(\"images\", resized_array)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "            # Augmentation\n",
    "            for i in range(number_of_augmentation):\n",
    "                augmented_array = augment_image(resized_array)\n",
    "                normalized_array = augmented_array/255.0 # Normalize the array\n",
    "                x.append(normalized_array)\n",
    "                y.append(classIndex)\n",
    "    cv2.destroyAllWindows()\n",
    "    return x, y, CATEGORIES\n",
    "\n",
    "x, y, CATEGORIES = PreProcess(img_size, datadir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3e06b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing\n",
    "X_train, x_test, Y_train, y_test = train_test_split(x, y, random_state=42)\n",
    "x = None\n",
    "y = None\n",
    "# Convert all the list to numpy array\n",
    "X_train = np.array(X_train).reshape(-1, img_size, img_size, 1)\n",
    "x_test = np.array(x_test).reshape(-1, img_size, img_size, 1)\n",
    "Y_train = np.array(Y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7919d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b834960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178692, 48, 48, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "817a5a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 46, 46, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 44, 44, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 21, 21, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 19, 19, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 19, 19, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 17, 17, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 64)          73792     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 16)          9232      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 500)               288500    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                25050     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 674,268\n",
      "Trainable params: 674,268\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(img_size, img_size, 1), activation=\"relu\"))\n",
    "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model.add(Conv2D(64, (3, 3), strides=2, activation=\"relu\"))\n",
    "model.add(Conv2D(128, (3, 3), activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv2D(128, (3, 3), activation=\"relu\"))\n",
    "model.add(Conv2D(64, (3, 3), strides=2, activation=\"relu\"))\n",
    "model.add(Conv2D(16, (3, 3), activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(len(CATEGORIES), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02d39ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_time(start, end):\n",
    "    total = end-start\n",
    "    mins, sec = 0, 0\n",
    "    if total >= 60:\n",
    "        mins = int(total/60)\n",
    "        sec = int(total-(mins*60))\n",
    "    else:\n",
    "        sec = int(total)\n",
    "    print (\"Training took {} mins and {} seconds!\".format(mins, sec))\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, verbose=1),\n",
    "    ReduceLROnPlateau(patience=3, verbose=1),\n",
    "    ModelCheckpoint('model-OCR.h5', verbose=1, save_best_only=True)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2e9928f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 128s 534ms/step - loss: 1.7279 - accuracy: 0.3576 - val_loss: 0.6102 - val_accuracy: 0.7962\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61021, saving model to model-OCR.h5\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 82s 517ms/step - loss: 0.4896 - accuracy: 0.8351 - val_loss: 0.1614 - val_accuracy: 0.9506\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61021 to 0.16140, saving model to model-OCR.h5\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 82s 519ms/step - loss: 0.2149 - accuracy: 0.9312 - val_loss: 0.0866 - val_accuracy: 0.9731\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.16140 to 0.08660, saving model to model-OCR.h5\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 82s 522ms/step - loss: 0.1371 - accuracy: 0.9560 - val_loss: 0.0601 - val_accuracy: 0.9800\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.08660 to 0.06014, saving model to model-OCR.h5\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 83s 523ms/step - loss: 0.1008 - accuracy: 0.9680 - val_loss: 0.0404 - val_accuracy: 0.9862\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.06014 to 0.04041, saving model to model-OCR.h5\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 83s 526ms/step - loss: 0.0780 - accuracy: 0.9754 - val_loss: 0.0352 - val_accuracy: 0.9882\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04041 to 0.03517, saving model to model-OCR.h5\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 84s 534ms/step - loss: 0.0674 - accuracy: 0.9789 - val_loss: 0.0247 - val_accuracy: 0.9926\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03517 to 0.02473, saving model to model-OCR.h5\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 85s 536ms/step - loss: 0.0542 - accuracy: 0.9828 - val_loss: 0.0262 - val_accuracy: 0.9904\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02473\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 85s 538ms/step - loss: 0.0497 - accuracy: 0.9843 - val_loss: 0.0239 - val_accuracy: 0.9922\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02473 to 0.02393, saving model to model-OCR.h5\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 86s 544ms/step - loss: 0.0423 - accuracy: 0.9867 - val_loss: 0.0176 - val_accuracy: 0.9945\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02393 to 0.01763, saving model to model-OCR.h5\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 86s 542ms/step - loss: 0.0375 - accuracy: 0.9881 - val_loss: 0.0116 - val_accuracy: 0.9963\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01763 to 0.01155, saving model to model-OCR.h5\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 86s 547ms/step - loss: 0.0320 - accuracy: 0.9900 - val_loss: 0.0106 - val_accuracy: 0.9962\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01155 to 0.01057, saving model to model-OCR.h5\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 85s 536ms/step - loss: 0.0295 - accuracy: 0.9907 - val_loss: 0.0162 - val_accuracy: 0.9947\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.01057\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 85s 536ms/step - loss: 0.0324 - accuracy: 0.9901 - val_loss: 0.0086 - val_accuracy: 0.9975\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01057 to 0.00858, saving model to model-OCR.h5\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 86s 545ms/step - loss: 0.0248 - accuracy: 0.9919 - val_loss: 0.0080 - val_accuracy: 0.9974\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00858 to 0.00800, saving model to model-OCR.h5\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 86s 547ms/step - loss: 0.0216 - accuracy: 0.9930 - val_loss: 0.0113 - val_accuracy: 0.9960\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00800\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 84s 535ms/step - loss: 0.0207 - accuracy: 0.9935 - val_loss: 0.0090 - val_accuracy: 0.9968\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00800\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 84s 532ms/step - loss: 0.0215 - accuracy: 0.9931 - val_loss: 0.0057 - val_accuracy: 0.9983\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00800 to 0.00573, saving model to model-OCR.h5\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 84s 533ms/step - loss: 0.0186 - accuracy: 0.9941 - val_loss: 0.0078 - val_accuracy: 0.9973\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00573\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 85s 535ms/step - loss: 0.0187 - accuracy: 0.9939 - val_loss: 0.0064 - val_accuracy: 0.9979\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00573\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 84s 532ms/step - loss: 0.0176 - accuracy: 0.9944 - val_loss: 0.0076 - val_accuracy: 0.9976\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00573\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 84s 531ms/step - loss: 0.0107 - accuracy: 0.9966 - val_loss: 0.0033 - val_accuracy: 0.9990\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00573 to 0.00332, saving model to model-OCR.h5\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 84s 532ms/step - loss: 0.0084 - accuracy: 0.9975 - val_loss: 0.0030 - val_accuracy: 0.9991\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00332 to 0.00303, saving model to model-OCR.h5\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 84s 531ms/step - loss: 0.0077 - accuracy: 0.9975 - val_loss: 0.0027 - val_accuracy: 0.9992\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00303 to 0.00271, saving model to model-OCR.h5\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 84s 533ms/step - loss: 0.0077 - accuracy: 0.9976 - val_loss: 0.0027 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00271\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 86s 543ms/step - loss: 0.0070 - accuracy: 0.9978 - val_loss: 0.0026 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00271 to 0.00258, saving model to model-OCR.h5\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 85s 538ms/step - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.0025 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00258 to 0.00249, saving model to model-OCR.h5\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 85s 540ms/step - loss: 0.0065 - accuracy: 0.9979 - val_loss: 0.0024 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00249 to 0.00240, saving model to model-OCR.h5\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 84s 535ms/step - loss: 0.0062 - accuracy: 0.9981 - val_loss: 0.0025 - val_accuracy: 0.9992\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00240\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 85s 535ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.0025 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00240\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 85s 535ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.0023 - val_accuracy: 0.9992\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00240 to 0.00233, saving model to model-OCR.h5\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 84s 532ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.0021 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00233 to 0.00213, saving model to model-OCR.h5\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 85s 536ms/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 0.0021 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00213 to 0.00212, saving model to model-OCR.h5\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 85s 537ms/step - loss: 0.0056 - accuracy: 0.9984 - val_loss: 0.0021 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00212 to 0.00211, saving model to model-OCR.h5\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 85s 536ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.0021 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00211 to 0.00206, saving model to model-OCR.h5\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 85s 540ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0021 - val_accuracy: 0.9994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00036: val_loss improved from 0.00206 to 0.00206, saving model to model-OCR.h5\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 85s 536ms/step - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.0021 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00206 to 0.00205, saving model to model-OCR.h5\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 85s 536ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.0021 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00205\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 84s 533ms/step - loss: 0.0055 - accuracy: 0.9982 - val_loss: 0.0021 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00205\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 84s 529ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.0021 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00205\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 84s 530ms/step - loss: 0.0055 - accuracy: 0.9982 - val_loss: 0.0021 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00205\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 84s 530ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.0021 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00205\n",
      "Epoch 00042: early stopping\n",
      "Training took 60 mins and 12 seconds!\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "history = model.fit(X_train, Y_train, batch_size = 1024, epochs=100, validation_split=0.1, callbacks=callbacks, verbose=1)\n",
    "end = time.time()\n",
    "track_time(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23afcf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history['accuracy']\n",
    "loss = history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b36f89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.plot(accuracy)\n",
    "ax1.set_title(\"Accuracy\")\n",
    "ax2.plot(loss)\n",
    "ax2.set_title('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea82766f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1862/1862 [==============================] - 11s 6ms/step - loss: 0.0026 - accuracy: 0.9991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0026011476293206215, 0.9991270303726196]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d20cb5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model\n",
    "model.save(\"model2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a6e4f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('dsenv': conda)",
   "language": "python",
   "name": "python3710jvsc74a57bd03de85ba066d17394542b6ba22a9c606e6ab49ee752c0de84f0bbe3e820d7ebf9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
